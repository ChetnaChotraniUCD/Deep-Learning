# Convolutional Neural Network

#Data Preprocessing -
#General Description: No categorical data here , as all values in pixel
#Split (80% and 20%) 10000 images of dogs in cats, 8000 images goes in Training set (4000 for dogs and 4000 for cats), 2000 goes in Test set
#Feature scaling required but completed in later part

# Part 1 - Building the CNN

#Sequential - To Initialize a neural network 
#Convolution 2D - Convolution step in which we add convolution layers. Images are  2D hence 2D used 
#MaxPooling2D - Pooling step 
#Flatten - Pooled feature maps - large flatten 
#Dense - adding layers in CNN

from keras.models import Sequential
from keras.layers import Convolution2D
from keras.layers import MaxPooling2D
from keras.layers import Flatten
from keras.layers import Dense

#Initializing CNN
classifier = Sequential()

#Step 1: Convolution
#nb_filters (Number of feature detectors - Creates the exact number of feature maps)
#nb_rows (Feature detector rows)
#Nb_columns (columns) 
#64 FD, 3 ROWS, 3 COLUMNS 
#border_mode - NOt so important 
#Input_Shape - force them to have same format , fixed size (Expected format)
#Colored images are going to convert to 3D Array
#Black and white images are going to convert to 2d array
#(3 - Number of channels for Colored images, 64 , 64, dimensions of 2D array in each channel)
#For Theano backend, code sequence for input_shape is 3,64,64
#For tensor flow backend, its 64,64,3
#Activation function = relu , to make sure we dont have negative pixel values so as to have no linearity in our model
classifier.add(Convolution2D(32, (3, 3), input_shape = (64, 64, 3), activation='relu', data_format='channels_first'))

#Step 2: Pooling
#pool_size = size of subtable that we slide (Generally 2*2)
classifier.add(MaxPooling2D(pool_size= (2,2)))

#Step 3: Flattening
classifier.add(Flatten())

#Step 4: Full connection (Classic ANN composed of some fully connected layer)
classifier.add(Dense(output_dim = 128 , activation='relu'))
#Sigmoid : since we have a binary outcome
classifier.add(Dense(output_dim = 1 , activation='sigmoid'))

#Step 5: Compiling CNN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

#Part 2 : fitting the CNN to images

#Image augmentation process to reduce overfitting , we are using keras documentation shortcut
#image recognizition requires a lot of images to give good performance results, but When we dont have enough images, we use image augmentation
#This is a trick as the shortcut will create many batches of our images and then each batch will apply some random transformations on a random selection of our
#images like rotating them, flipping, shifting or even shearing them.Eventually what we will get during the training is many more diverse images inside 
#these batches and therefore a lot more material to train. Called image augmentation because the amount of our training images is augmented

from keras.preprocessing.image import ImageDataGenerator

#Rescale compulsory and it corresponds to the feature scaling part of preprocessing
#shearing : pixels moved over geometry
#Rescales 1/255, so that they have values between 0 and 1

train_datagen = ImageDataGenerator(
        rescale=1./255,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1./255)

training_set = train_datagen.flow_from_directory('dataset/training_set',
                                                    target_size=(64, 64),
                                                    batch_size=32,
                                                    class_mode='binary')

test_set = test_datagen.flow_from_directory('dataset/test_set',
                                                        target_size=(64, 64),
                                                        batch_size=32,
                                                        class_mode='binary')

#Fits and tests performance
#steps_per_epoch : number of images we have in our training set
#validation_steps : number of images we have in our test set
classifier.fit_generator(training_set,
                        steps_per_epoch=8000,
                        epochs=1,
                        validation_data=test_set,
                        validation_steps=2000)

